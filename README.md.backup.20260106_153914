# ğŸ§  Enter The Black Box

A Machine Learning Robustness Lab for Evolving Cyber Attacks
(Stress-Testing ML Models for Cybersecurity Drift, Mutation & Resilience)

---

## ğŸš€ Overview
**Enter The Black Box** is a modular research platform designed to analyze and improve the **robustness of ML-based cybersecurity models**. It enables testing how intrusion detection systems (IDS/NDR) behave under **data drift, adversarial noise, and feature corruption**.

---

## ğŸ§© Key Capabilities
- **Baseline Training** â€” Train ML models (RandomForest, XGBoost) on IDS datasets like NSL-KDD.
- **Attack Mutation Engine** â€” Simulate adversarial noise, drift, and missing features.
- **Drift Detection** â€” Detect and visualize data drift using Evidently AI and Alibi Detect.
- **Robustness Evaluation** â€” Measure performance degradation between baseline and mutated datasets.
- **Explainability** â€” Understand feature importance and model behavior with SHAP.
- **Visualization Tools** â€” Static (Matplotlib) and interactive (Streamlit) dashboards for analysis.
- **Experiment Tracking** â€” Full MLflow integration for logging models, metrics, and reports.

---

## ğŸ— Project Structure
```
Enter-The-Black-Box/
â”‚
â”œâ”€â”€ data/                     # Datasets
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py        # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ baseline_model.py     # Baseline model training and evaluation
â”‚   â”œâ”€â”€ mutation_engine.py    # Data mutation and drift simulation
â”‚   â”œâ”€â”€ drift_detector.py     # Evidently + Alibi drift detection
â”‚   â”œâ”€â”€ robustness_eval.py    # Compare baseline vs mutated results
â”‚   â”œâ”€â”€ explainability.py     # SHAP explainability
â”‚   â”œâ”€â”€ report_generator.py   # MLflow report generation
â”‚   â”œâ”€â”€ dashboard.py          # Streamlit dashboard
â”‚   â”œâ”€â”€ run_experiment.py     # Full automation pipeline
â”‚   â””â”€â”€ visualize_results.py  # Offline result visualization
â”‚
â”œâ”€â”€ notebooks/                # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ 01_baseline.ipynb
â”‚   â”œâ”€â”€ 02_mutations.ipynb
â”‚   â”œâ”€â”€ 03_drift_tests.ipynb
â”‚   â””â”€â”€ 04_shap_analysis.ipynb
â”‚
â”œâ”€â”€ outputs/                  # Generated reports and plots
â”œâ”€â”€ mlruns/                   # MLflow tracking logs
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ How to Use

### ğŸ§  1. Install Requirements
```bash
pip install -r requirements.txt
```

### ğŸ“Š 2. Run an Automated Experiment
```bash
python src/run_experiment.py --data data/KDDTest+.arff --model rf
```

This will:
1. Load and preprocess the dataset.
2. Train a baseline model.
3. Mutate the data (noise, drift, feature drop).
4. Evaluate drift and robustness.
5. Compute SHAP explainability.
6. Generate and save experiment reports and plots in `outputs/`.

### ğŸ–¥ï¸ 3. Launch Streamlit Dashboard
```bash
streamlit run src/dashboard.py
```
Explore the full lab visually â€” upload data, mutate it, visualize SHAP, and generate reports.

### ğŸ“ˆ 4. Visualize Saved Results
```bash
python src/visualize_results.py
```
Displays saved performance charts, drift summaries, and SHAP plots.

---

## ğŸ§ª Supported Datasets
- NSL-KDD (KDDTrain+, KDDTest+.arff)
- CIC-IDS2017 *(planned Phase 2)*
- UNSW-NB15 *(planned Phase 2)*

---

## ğŸ“˜ Roadmap
**Phase 1 (âœ… Done):** Core pipeline, dashboards, and drift analysis.  
**Phase 2 (ğŸš§ Next):** Modern IDS datasets (CIC-IDS2017, UNSW-NB15).  
**Phase 3 (ğŸ”¬ Planned):** Adversarial sample generation (GAN/VAE).  
**Phase 4 (â˜ï¸ Planned):** Real-time drift monitoring and deployment.

---

## ğŸ“„ License
MIT â€” Free for education and research.

---

## âœ‰ï¸ Contact
Created by **Kapitan Lev âš“**  
AI-powered Cyber Analyst & ML Researcher